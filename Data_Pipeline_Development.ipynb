{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMi8785sKqhtQ1pIS0l6z8k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2spoorthy/2spoorthy/blob/main/Data_Pipeline_Development.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import all required tools"
      ],
      "metadata": {
        "id": "7QShYA1WFk5d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "U_QxlZtfB346"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script follows an ETL (Extract, Transform, Load) process: it extracts data from a CSV, transforms it using scaling (StandardScaler) and encoding (OneHotEncoder), and saves the processed data back to a CSV. It uses `ColumnTransformer` and `Pipeline` to streamline preprocessing for machine learning."
      ],
      "metadata": {
        "id": "CkhmlD17Fs-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_data(file_path):\n",
        "    \"\"\"Extract raw data from a CSV file.\"\"\"\n",
        "    return pd.read_csv(file_path)\n",
        "\n",
        "def transform_data(df):\n",
        "    \"\"\"Transform data using scaling and encoding.\"\"\"\n",
        "    numerical_features = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "    categorical_features = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "    numeric_transformer = StandardScaler()\n",
        "    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numerical_features),\n",
        "            ('cat', categorical_transformer, categorical_features)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
        "    transformed_data = pipeline.fit_transform(df)\n",
        "\n",
        "    return transformed_data, pipeline\n",
        "\n",
        "def load_data(transformed_data, output_path):\n",
        "    \"\"\"Load the transformed data into a CSV file.\"\"\"\n",
        "    pd.DataFrame(transformed_data).to_csv(output_path, index=False)"
      ],
      "metadata": {
        "id": "Oo1pi9zUDPzv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script reads a CSV file (Salary_Data.csv), preprocesses it by scaling numerical features and encoding categorical ones, and saves the cleaned data into processed_data.csv, printing status messages at each step."
      ],
      "metadata": {
        "id": "n_0Z-aTkGTiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    input_file = '/content/Salary_Data.csv'\n",
        "    output_file = 'processed_data.csv'\n",
        "\n",
        "    # Extract\n",
        "    df = extract_data(input_file)\n",
        "    print(\"Data extracted successfully\")\n",
        "\n",
        "    # Transform\n",
        "    transformed_data, _ = transform_data(df)\n",
        "    print(\"Data transformed successfully\")\n",
        "\n",
        "    # Load\n",
        "    load_data(transformed_data, output_file)\n",
        "    print(\"Data loaded successfully\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkR3zppeDgf9",
        "outputId": "056becd3-0e6b-432c-fa59-2bf33c0da850"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data extracted successfully\n",
            "Data transformed successfully\n",
            "Data loaded successfully\n"
          ]
        }
      ]
    }
  ]
}